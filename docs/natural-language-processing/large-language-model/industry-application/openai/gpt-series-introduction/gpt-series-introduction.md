# GTP系列介绍

* [返回上层目录](../openai.md)

OpenAI的大模型自GPT3起都没有开源

GPT-1和GPT-2都有公开的预训练模型和源代码可用，但是这些模型的训练数据和训练代码不是公开的。这些模型的源代码可以在GitHub上找到。

至于GPT-3，目前它仍然是一个私有的商业产品，只有少数被邀请的合作伙伴才能访问它。OpenAI表示他们不会公开GPT-3的预训练模型和源代码，但是他们提供了一些API接口，供开发人员和研究人员使用。

chatgpt应该算是第3代nlp技术了。前两代是预训练+微调，然后是自监督+prompt，现在的是利用了RHLF、instruct等技术。



GPT-2就是在Transformer基础之上的大模型。GPT-1为亿级参数，GPT-2为15亿级参数，GPT-3为1750亿级参数。GPT-3.5又有多个版本，包括具有13亿参数的InstructGPT、1750亿参数的ChatGPT和60亿参数的Codex（用于代码生成），以及GPT-3.5 Turbo。其中，最为著名的是InstructGPT和ChatGPT，二者还在训练数据集上有所不同。当ChatGPT大获成功后，OpenAI又将集大成的GPT-3.5 Turbo接入ChatGPT API，成本更低、速度更快、功能更全，不仅能生成对话还能生成代码。





与GPT系列不同，InstructGPT和ChatGPT引入了人类反馈，也就是基于人类反馈的增强学习。InstructGPT和ChatGPT还引入了人类数据标注员，用人工的方式对模型进行精调。这些反馈都让模型的输出结果向着人类期望的访发展，这就是ChatGPT能够惊艳全球的重要原因。



| 模型    | 发布时间 | 3          | 预训练数据量 |
| ------- | -------- | ---------- | ------------ |
| GPT-1   | 201806   | 0.117B     | 约5GB        |
| GPT-2   | 201902   | 1.5B       | 约40G        |
| GPT-3   | 202005   | 175B       | 45TB         |
| ChatGPT | 202211   | 100B级别？ | 百T级？      |

第一代GPT-1，训练参数量为1.2亿个，数据库规模为5GB。

第二代GPT-2，训练参数量为15亿个，数据库规模为40GB。

第三代GPT-3，训练参数量飞跃至1750亿个，数据库规模达到45TB。



===

nanogpt的实现，开源





[从 GPT 到 ChatGPT 的演进与应用思考](https://mp.weixin.qq.com/s/3Pr82xKpZ7mAWQcxPPB1xA)

[paper: A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT](https://arxiv.org/abs/2302.09419)

这里[ChatGPT背后的大模型技术如何炼？MSU等最新《预训练基础模型综述》，97页pdf全面阐述BERT到ChatGPT历史脉络](https://www.zhuanzhi.ai/vip/f9ef3cea409e4e561fa87db1821f57d0)提到了上述论文，预训练基础模型(PFMs)被视为具有不同数据模态的各种下游任务的基础。预训练的基础模型，如BERT、GPT-3、MAE、DALLE-E和ChatGPT，在大规模数据上进行训练，为广泛的下游应用提供了合理的参数初始化。**PFMs背后的预训练思想在大型模型的应用中起着重要的作用。**作为一种迁移学习范式，预训练通过冻结和微调技术应用于计算机视觉，显示出良好的性能。词向量在自然语言处理中也可以看作是修饰词的一

[GPT系列的数据集之谜](https://mp.weixin.qq.com/s/p0s6FmEof2gkb0jrHBo3JA)

[ChatGPT背后的经济账]

OneFlow发布了《ChatGPT背后的经济账》，其作者从经济学视角推导了训练大型语言模型的成本。本文作者则整理分析了2018年到2022年初从GPT-1到Gopher的相关大型语言模型的所有数据集相关信息，希望帮助有志于开发“类ChatGPT”模型的团队少走一步弯路。



[GPT-4 的实际体验如何？和之前相比有哪些明显提升？](https://www.zhihu.com/question/589641645/answer/2936696934)

从 ChatGPT Plus 发布第一天就开始重度使用，刚刚和新发布的 GPT-4 进行了 20 多轮对话，来简单介绍下这几个模型背后的技术，并且分享下感受。

GPT 在发展历程中，一共经历了 4 个阶段，分别是 1、2、3、4。这几个阶段分别进行了不同思考，就好像三体中提出的几个公理，都很有意思。

第一代模型 GPT-1，当时的论文叫做「通过生成式预训练模型，来提升对于语言本身的理解」Improving Language Understanding by Generative Pre-Training：其中的 Generative Pre-Training，便是现在 GPT 的来源。在传统机器学习中，学者们更喜欢用标注好的东西来进行机器学习。比如我心情真棒（正面情绪），括号中的就是一个标注。

在这篇论文中，OpenAI 在想，这个世界上有如此丰富的预料，但大部分都是没有被标注的数据。虽然不好用，但我们可以通过对其学习，只要学的足够多，我们就可以培养模型对于语言表达的理解能力。

第二代模型 GPT-2, OpenAI 手头拿着学会了语言模型，但的确是没什么用。他们就在思考，现在是能理解文字了，但是这什么也不能干呀，这个模型到底能做什么呢？在大量试验后，他们发现，人们以往认为机器学习中的自然语言处理，也许不一定需要划分成很多很多的子任务。

如果语言本身包含了这些信息，那么各类任务，都应该可以被统一学习。比如，传统思路中，人们认为翻译，你必须要有中英文对应，来进行学习。但实际上，互联网上充斥着这样的提问，比如：有没有小伙伴知道香水有法语怎么表达最贴切，于是就有人回复 Parfum。

这段话既包含了中文原文，又包含了网友智慧所提出的法语翻译。既然我们学了这么多语料库，那么翻译，应该是自然而然就能学会的，不需要单独进行。同理，归类、找相似性、等等工作，都应该可以被大模型统一的表达。

因此提出了第二篇论文「大语言模型，是无需监督的多任务学习者」Language Models are Unsupervised Multitask Learners。即它可以自己学会做很多很多事，不用监督瞪大眼睛看它学对没，先学再说。学会的知识，应该是具备普适性的，能够处理很多任务。

第三代模型 GPT-3，也就是大家熟悉的 ChatGPT 的前身。这个阶段，OpenAI 在冲着充分堆料，已经学会很多内容的模型陷入深深思考。既然它通过学习大量数据，掌握了人类语言，又理论上来说具备多任务的处理能力，怎么才能让它用出来呢？

传统思路中，机器学习模型需要进行针对性优化，来提升对于不同任务的表现。OpenAI 看了看已经被训练的巨大的模型，每次针对性优化，其实是不现实的，于是放弃了这条路。转而进行下一个重要思考。既然人类可以通过举例学习，语言模型应该也可以。

因此提出了第三篇论文「大语言模型，通过几个例子就能学会你要他做什么」Language Models are Few-Shot Learners。比如，如果你和一个智慧球举例，说从一数到一百，1，2，3... 它就应该能根据你的例子，去完成内容补全。甚至多学学，不用举例子也行。

这便是前三代模型的发展背景。截至目前为止，OpenAI 手头的这几个模型一个比一个大，但是最大的问题在于，它们擅长生成符合人类说话风格的胡言乱语，但是这不符合和人类和他们交流的习惯。

OpenAI 首先想到的就是服务程序员的代码，如果学会了世界上所有代码，应该能给人带来帮助。于是在此尝试下，推出了包含代码数据集的 OpenAI Codex，并且把这个包含了代码和新数据训练的模型叫做 GPT-3.5。这还是不好用，因为在人类世界中，我说上句你补充下句意义不大。

重点还是交流，交流涉及沟通，于是便训练了基于 GPT-3.5 的对话模型 InstructGPT。这个模型中用到了很多人工反馈，对问答方式和表述方式进行更符合人性的指导。而我们现在最熟悉，最出圈的 ChatGPT，就是 InstructGPT 的一个更适合大众的姊妹模型变种。

在我的工作中，ChatGPT 已经成为不可或缺的一部分，它带来的生产效率提升可以轻松达到 20 倍以上。从我个人角度出发，觉得这项技术，会深刻改变人类学习、教育、做事的方式。新的教育体系，若思维固步自封，同时学生不尽快掌握 ChatGPT，那么从起点上就会落后非常多。

就像互联网时代前和后，新能源汽车时代前和后，时代已经被刻骨，全面，深刻的改变了。

它不会降低知识的门槛，也就是说，如果一个人并不熟悉某个领域，指望就靠 ChatGPT 用处不大。这就好像一个小朋友问围棋之神，我如何才能变成围棋大佬一样，帮助有限。但假如你已经是个业余棋手，有了 ChatGPT 会无限拉进你与世界上顶尖棋手的距离。

即便已经强如 ChatGPT，还是存在一些问题。比如，在复杂语句的推理能力，其实 ChatGPT 还有成长空间。比如倘若你的描述充满非常复杂的逻辑关系，那么 ChatGPT 有可能会遗漏其中的部分要求。这个问题，在我今晚测试的 GPT-4 中，就完全解决啦。