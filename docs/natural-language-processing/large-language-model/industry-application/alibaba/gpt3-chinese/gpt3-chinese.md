# 中文GPT-3

* [返回上层目录](../alibaba.md)



可惜，GPT-3至今也没有开源，未来也大概率不会开源了。要从头训这么一个1750亿参数的大型生成式语言模型，难度非常大。

有人可能要说，那我们训一个小点的模型，比如百亿参数的，可行吗？

目前来看不可行。**AI的表现并非随着模型规模增加而线性增加，而是在参数规模超过特定临界值后显著提升，甚至涌现出小模型不具备的能力。** 比如论文表明，模型的规模至少要达到620亿参数量后，才可能训练出来思维链（Chain-of-Thought，CoT）能力。如下图所示：

而像Truthful（可信的）这种能力，甚至ChatGPT/GPT-3这样的模型规模都是不够的，要达到2800亿参数量才能涌现出这样的能力。是的，复现和超越ChatGPT，目前来看没有捷径，必须一步一步来，首先要先把GPT-3搞定。

## 国内有人真正复刻了GPT-3？

是的，有且只有一家，阿里达摩院，他们从小到大（从base到175B），全面、完整地复刻了GPT-3，并且开放在魔搭社区上。

https://modelscope.cn/models/damo/nlp_gpt3_text-generation_chinese-large/summary

达摩院的复刻不是没有来由的，他们应该在大模型各个方向都进行了探索，布局完整。早在2021年4月就发布了首个中文语言大模型PLUG（当时参数是270亿）。该模型首次在中文语言理解榜单CLUE上面，以86.685分的成绩超越人类。

同年10月份，达摩院还探索实现了**10万亿参数模型——M6**，达摩院团队通过大量的底层优化和算法设计，仅仅使用了512卡便实现了这一庞大的模型工程。此前，M6模型将AI图片生成清晰度从OpenAI DALL·E的256×256成功提升到了1024×1024，效果十分惊艳。

M6模型的发布引发了国内外的大量关注，其中，OpenAI前政策主管Jack Clark公开点评：“这个模型的规模和设计都非常惊人。这看起来像是众多中国的AI研究组织逐渐发展壮大的一种表现。”

从达摩院的经历我们基本可以判断：**如果一个研发团队此前没有训练过千亿级别的大型语言模型，那就很难在可以接受的时间窗口内训练出真正具备生产力价值的类ChatGPT模型。**